#A prototype does simple analysis on Twitter stream data
This application does a simple real-time analysis on Twitter stream data.  It identifies the most popular hashtags or languages being used among the tweets (exclude the retweeted) since it started receiving the data or since the last minute, provides the real-time update on the browser.
It uses Spark streaming, Spark SQL, Akka, Play, WebSocket.  Most of the application is written in Scala except presenting the report is written in JavaScript.

##Application structure
* Tweet collector and analyzer
* Web application

##Tweet collector and analyzer

###Tweets collection
Collects tweets from the Twitter stream, converts them to Json format and stores to files.  It uses Spark streaming API and Twitter API.  4 Twitter tokens are needed to collect the tweets.  They are generated by https://apps.twitter.com/.  Twitter stream processing should be lightweight to avoid out of memory.  Therefore it only saves the the tweets as file to a directory and let another process to process the files.  https://groups.google.com/forum/#!topic/twitter4j/9yeArHoQRoU

###Data analysis
It uses Spark streaming to process the data from the files.  Since the data is stored as Json format, it is structured and therefore can be loaded and queried more efficiently by using Spark SQL.  Spark streaming provides lots of handy API such as ```updateStateByKey``` and ```reduceByKeyAndWindow``` to find out the most popular hashtags since the data reception and last minute respectively.

The analyzed data will be sent to an Akka actor hosted by the web application running on another JVM.

##Web application
It is a thin application written using Play!  It is used for presentation only.

###Web socket
Play! supports web socket.  Since 2.3, it even supports the web sockets handling with actors.  In this simple application, the same actor is used for receiving analysed data and sending this data to browser via web sockets.  In a large scale distributed system, numerous actors can be generated to distribute the load.

###Json data
The web application sends Json-formated data to the browser via web socket.  I have written an open-sourced pickler https://github.com/fairfax-newsnow/adonis-pickler that generates the formatters to serialize some complicated data type including generics.  Since this application only involves 2 simple data types.  I use Play-json API to write a few Json formatters quickly to make the job done.  When there are many different kinds of complicated data types be serialized/de-serialized, this pickler is preferred.

##Technical issues encountered
* Incompatibility issues occur when integrating Play! and Spark streaming.  One possibility is the depended common libraries have different versions.  Resolving this issue causes another problem of ClassCastException.  As a workaround, the web component and the data analyzer are splitted into 2 application running on 2 JVMs.
* "Address already in use" when the applications are started with a fixed port # to the ```akka.remote.netty.tcp.port```  As a workaround, 0 is assigned such that a random port # is generated to avoid the problem.

##How to execute
* On sbt shell, enter ```project frontend``` and then ```run```.
* On browser, enter ```localhost:9000``` and then click the link to "Most popular hashtags & languages"
* On the frontend sbt shell, copy the akka.remote.netty.tcp.port #, say, 50408, from the shell.
* Open another sbt shell, enter ```project tweetAnalyzer``` and then ```runMain spark.TweetsAnalyzer akka.tcp://twitter-analytics-system@127.0.0.1:50408/user/twitterAnalytics```.  Note that 50408 is the copied port #.

Here is the captured screen

![alt text](/TwitterAnalytics.png)

##Side-effects
some side-effects still need, such as .cache(), the way to create a spark conf, 
